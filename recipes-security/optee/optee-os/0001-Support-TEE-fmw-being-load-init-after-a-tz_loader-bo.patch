From 42366b3cadd0b125f2ac9cd0ffd7ea150422087c Mon Sep 17 00:00:00 2001
From: Christophe Priouzeau <christophe.priouzeau@linaro.org>
Date: Wed, 21 Jan 2015 19:04:22 +0100
Subject: [PATCH] Support TEE fmw being load/init after a tz_loader boot (v3) 
 sequence

Map/unmap coherent RAM during TEE init sequence.
Synchronize primary and secondaries during TEE inits.
Restrict TEE services until all expected cores are booted.

TEE firmware generates 2 binary images to be loaded at specific
location in target RAM before NSec world can run TEE inits
through ARM TZ "smc" instuctions.

At TEE entry point, if the core is running in SVC mode, TEE assumes
TEE is init a ARM core reset release. If the running mode is MON,
TEE assumes ARM core TZ part was booted with "tz_loader" sequence,
and TEE inits are requested from an already running non secure OS.
TEE expects NSec enters TEE on core #0 and some returned from, NSec
must enters TEE on all secondary cores, if any.

Enhance somewhat core map_memarea() to allow mapping several
compatible regions (coarse mapping) inside the same memory area.
Requester must provide the expected L2 mmu table. Unmap
unmaps the full memory area, not only a regions inside that area.

Signed-off-by: etienne carriere <etienne.carriere@st.com>
---
 core/arch/arm32/include/arm32.h               |   4 +
 core/arch/arm32/include/kernel/runtime_init.h |  40 ++++++
 core/arch/arm32/include/mm/core_mmu.h         |  16 ++-
 core/arch/arm32/include/sm/sm.h               |   2 +
 core/arch/arm32/mm/core_mmu.c                 | 123 ++++++++++++++----
 core/arch/arm32/plat-stm/conf.mk              |   2 +-
 core/arch/arm32/plat-stm/link.mk              |   7 +-
 core/arch/arm32/plat-stm/main.c               |   7 ++
 core/arch/arm32/plat-stm/platform_config.h    |   5 +
 core/arch/arm32/plat-stm/runtime_init.c       | 172 ++++++++++++++++++++++++++
 core/arch/arm32/plat-stm/sub.mk               |   4 +
 core/arch/arm32/plat-stm/tz-template.lds      |   5 +
 core/arch/arm32/plat-stm/tz_runtime_init.S    | 154 +++++++++++++++++++++++
 core/arch/arm32/plat-stm/tz_sinit.S           |   6 +
 core/arch/arm32/plat-stm/tzl_cc_mon.S         |  67 ++++++++++
 core/arch/arm32/sm/sm_asm.S                   |   2 +-
 16 files changed, 590 insertions(+), 26 deletions(-)
 create mode 100644 core/arch/arm32/include/kernel/runtime_init.h
 create mode 100644 core/arch/arm32/plat-stm/runtime_init.c
 create mode 100644 core/arch/arm32/plat-stm/tz_runtime_init.S
 create mode 100644 core/arch/arm32/plat-stm/tzl_cc_mon.S

diff --git a/core/arch/arm32/include/arm32.h b/core/arch/arm32/include/arm32.h
index 093ed79..df23e0d 100644
--- a/core/arch/arm32/include/arm32.h
+++ b/core/arch/arm32/include/arm32.h
@@ -245,6 +245,10 @@ static inline void dsb(void)
 {
 	asm volatile ("dsb");
 }
+static inline void dmb(void)
+{
+	asm volatile ("dmb");
+}
 
 static inline uint32_t read_contextidr(void)
 {
diff --git a/core/arch/arm32/include/kernel/runtime_init.h b/core/arch/arm32/include/kernel/runtime_init.h
new file mode 100644
index 0000000..e5a69cc
--- /dev/null
+++ b/core/arch/arm32/include/kernel/runtime_init.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (c) 2014, STMicroelectronics International N.V.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef RUNTIME_INIT_H
+#define RUNTIME_INIT_H
+
+#include <sm/sm.h>
+
+struct sm_nsec_ctx *main_init_get_nsec_ctx(void);
+unsigned long main_init_is_primary_inited(void);
+unsigned long main_init_is_completed(void);
+void main_init_completed(void);
+
+void main_init_save_nsec_ctx(void);
+
+#endif
diff --git a/core/arch/arm32/include/mm/core_mmu.h b/core/arch/arm32/include/mm/core_mmu.h
index ff1b3a1..e23f4db 100644
--- a/core/arch/arm32/include/mm/core_mmu.h
+++ b/core/arch/arm32/include/mm/core_mmu.h
@@ -37,7 +37,14 @@
  * @size:  memory area size in bytes
  * @va:    virtual start address (0 if memory is not mapped)
  * @region_size: size of the mapping region used (4k, 64K, 1MB)
+ * @area_l2: L2 table is mapping a region in already maped area
+ * @area_pa: region mapping inside an area: full area start addr
+ * @area_size: region mapping inside an area: full area size
  * @secure: true if memory area in inside a A9 secure area
+ * @cached: if to be maped cache (IWOW)
+ * @device: if to be maped as device memory
+ * @rw: if to be mapped as device memory
+ * @exec: if to be mapped as executable memory
  */
 struct map_area {
 	unsigned int type;
@@ -45,7 +52,10 @@ struct map_area {
 	size_t size;
 	/* below here are core_mmu.c internal data */
 	unsigned int va;
-	unsigned int region_size;
+	size_t region_size;
+	unsigned int area_pa;
+	size_t area_size;
+	uint32_t *area_l2;
 	bool secure;
 	bool cached;
 	bool device;
@@ -81,7 +91,9 @@ enum teecore_memtypes {
 extern unsigned long default_nsec_shm_paddr;
 extern unsigned long default_nsec_shm_size;
 
-uint32_t core_map_area_flag(void *p, size_t l);
+void map_memarea(struct map_area *map, uint32_t *ttb);
+void unmap_memarea(struct map_area *map, uint32_t *ttb);
+
 void core_init_mmu_tables(void);
 void core_init_mmu_regs(void);
 
diff --git a/core/arch/arm32/include/sm/sm.h b/core/arch/arm32/include/sm/sm.h
index d1838a8..9c0a04c 100644
--- a/core/arch/arm32/include/sm/sm.h
+++ b/core/arch/arm32/include/sm/sm.h
@@ -84,7 +84,9 @@ struct sm_sec_ctx {
 };
 
 /* Returns storage location of non-secure context for current CPU */
+struct sm_nsec_ctx *sm_get_tmp_nsec_ctx(void);
 struct sm_nsec_ctx *sm_get_nsec_ctx(void);
+size_t sm_get_nsec_ctx_size(void);
 
 /* Returns storage location of secure context for current CPU */
 struct sm_sec_ctx *sm_get_sec_ctx(void);
diff --git a/core/arch/arm32/mm/core_mmu.c b/core/arch/arm32/mm/core_mmu.c
index dfdea5c..8b0815e 100644
--- a/core/arch/arm32/mm/core_mmu.c
+++ b/core/arch/arm32/mm/core_mmu.c
@@ -150,13 +150,20 @@ static bool memarea_not_mapped(struct map_area *map, void *ttbr0)
 	return true;
 }
 
+/* Map a signle coherent 4KB area inside a Section aligned area */
 static paddr_t map_page_memarea(struct map_area *map)
 {
-	uint32_t *l2 = core_mmu_alloc_l2(map);
+	size_t pg_start;
 	size_t pg_idx;
 	uint32_t attr;
+	int l2_alloc = 0;
 
-	TEE_ASSERT(l2);
+	if (!map->area_l2) {
+		map->area_l2 = core_mmu_alloc_l2(map);
+		l2_alloc = 1;
+	}
+
+	TEE_ASSERT(map->area_l2);
 
 	attr = SMALL_PAGE_SMALL_PAGE | SMALL_PAGE_SHARED;
 
@@ -176,33 +183,45 @@ static paddr_t map_page_memarea(struct map_area *map)
 		attr |= SMALL_PAGE_NO_EXEC;
 
 	/* Zero fill initial entries */
-	pg_idx = 0;
-	while ((pg_idx * SMALL_PAGE_SIZE) < (map->pa & SECTION_MASK)) {
-		l2[pg_idx] = 0;
-		pg_idx++;
+	if (l2_alloc) {
+		pg_idx = 0;
+		while ((pg_idx * SMALL_PAGE_SIZE) < (map->pa & SECTION_MASK)) {
+			map->area_l2[pg_idx] = 0;
+			pg_idx++;
+		}
+	} else {
+		pg_idx = (map->pa & SECTION_MASK) >> SMALL_PAGE_SHIFT;
 	}
 
 	/* Fill in the entries */
-	while ((pg_idx * SMALL_PAGE_SIZE) < map->size) {
-		l2[pg_idx] = ((map->pa & ~SECTION_MASK) +
+	pg_start = pg_idx;
+	while (((pg_idx - pg_start) * SMALL_PAGE_SIZE) < map->size) {
+		if (!l2_alloc && map->area_l2[pg_idx] != 0)
+			panic();
+
+		map->area_l2[pg_idx] = ((map->pa & ~SECTION_MASK) +
 				pg_idx * SMALL_PAGE_SIZE) | attr;
 		pg_idx++;
 	}
 
 	/* Zero fill the rest */
-	while (pg_idx < ROUNDUP(map->size, SECTION_SIZE) / SMALL_PAGE_SIZE) {
-		l2[pg_idx] = 0;
-		pg_idx++;
+	if (l2_alloc) {
+		while (pg_idx < ROUNDUP(map->size, SECTION_SIZE) / SMALL_PAGE_SIZE) {
+			map->area_l2[pg_idx] = 0;
+			pg_idx++;
+		}
 	}
 
-	return (paddr_t)l2;
+	return (paddr_t)map->area_l2;
 }
 
 /*
-* map_memarea - load mapping in target L1 table
-* A finer mapping must be supported. Currently section mapping only!
-*/
-static void map_memarea(struct map_area *map, uint32_t *ttb)
+ * map_memarea - load mapping in target L1 table
+ * A finer mapping must be supported. Currently section mapping only!
+ * A small page mapping supports mapping only 1several contiguous area in
+ * the same "area_size" section-aligned map_area.
+ */
+void map_memarea(struct map_area *map, uint32_t *ttb)
 {
 	size_t m, n;
 	uint32_t attr;
@@ -221,6 +240,8 @@ static void map_memarea(struct map_area *map, uint32_t *ttb)
 	case SMALL_PAGE_SIZE:
 		region_size = SMALL_PAGE_SIZE;
 		region_mask = SMALL_PAGE_MASK;
+		if (!map->area_size || (map->area_size & SECTION_MASK))
+			panic();
 		break;
 	default:
 		panic();
@@ -254,9 +275,25 @@ static void map_memarea(struct map_area *map, uint32_t *ttb)
 
 		pa = map->pa;
 	} else {
+
 		attr = SECTION_PT_PT;
 		if (!map->secure)
 			attr |= SECTION_PT_NOTSECURE;
+
+		/* is wide area already under same L2 mapping or none ? */
+		if (map->area_l2) {
+			paddr_t pa2 = (paddr_t)map->area_l2;
+
+			m = (map->pa >> SECTION_SHIFT);
+			n = ROUNDUP(map->area_size, SECTION_SIZE) >> SECTION_SHIFT;
+			while (n--) {
+				if (ttb[m] && (ttb[m] != (pa2 | attr)))
+					panic();
+				m++;
+				pa2 += TEE_MMU_L2_SIZE;
+			}
+		}
+
 		pa = map_page_memarea(map);
 	}
 
@@ -264,14 +301,58 @@ static void map_memarea(struct map_area *map, uint32_t *ttb)
 
 	m = (map->pa >> SECTION_SHIFT);
 	n = ROUNDUP(map->size, SECTION_SIZE) >> SECTION_SHIFT;
+	if (ttb[m] == 0) {
+		while (n--) {
+			ttb[m] = pa | attr;
+			m++;
+			if (region_size == SECTION_SIZE)
+				pa += SECTION_SIZE;
+			else
+				pa += TEE_MMU_L2_SIZE;
+		}
+	}
+}
+
+/*
+ * unmap_memarea - unmap in target L1 table
+ * A finer mapping must be supported. Currently section mapping only!
+ * If small page mapping is request, the full section-aligned area is unmapped.
+ */
+void unmap_memarea(struct map_area *map, uint32_t *ttb)
+{
+	size_t m, n;
+	uint32_t region_mask;
+
+	TEE_ASSERT(map && ttb);
+
+	switch (map->region_size) {
+	case 0:	/* Default to 1MB section mapping */
+	case SECTION_SIZE:
+		region_mask = SECTION_MASK;
+		break;
+	case SMALL_PAGE_SIZE:
+		region_mask = SMALL_PAGE_MASK;
+		break;
+	default:
+		panic();
+	}
+
+	/* invalid area confing */
+	if (((map->pa + map->size - 1) < map->pa) ||
+	    !map->size || (map->size & region_mask) ||
+	    (map->pa & region_mask))
+		panic();
+
+	m = (map->pa >> SECTION_SHIFT);
+	n = ROUNDUP(map->size, SECTION_SIZE) >> SECTION_SHIFT;
 	while (n--) {
-		ttb[m] = pa | attr;
+		ttb[m] = 0;
 		m++;
-		if (region_size == SECTION_SIZE)
-			pa += SECTION_SIZE;
-		else
-			pa += TEE_MMU_L2_SIZE;
 	}
+
+	if (map->area_l2)
+		memset(map->area_l2, 0,
+			(map->area_size >> SMALL_PAGE_SHIFT) * sizeof(uint32_t));
 }
 
 /* load_bootcfg_mapping - attempt to map the teecore static mapping */
diff --git a/core/arch/arm32/plat-stm/conf.mk b/core/arch/arm32/plat-stm/conf.mk
index f9ce6e6..3ecc0a7 100644
--- a/core/arch/arm32/plat-stm/conf.mk
+++ b/core/arch/arm32/plat-stm/conf.mk
@@ -29,7 +29,7 @@ ifndef CFG_DDR_TEETZ_RESERVED_SIZE
 $(error "CFG_DDR_TEETZ_RESERVED_SIZE should be set from system_config.in")
 endif
 
-core-platform-cppflags += -DCONFIG_TEE_GDB_BOOT
+#core-platform-cppflags += -DCONFIG_TEE_GDB_BOOT
 CFG_NO_TA_HASH_SIGN ?= y
 
 ifeq ($(PLATFORM_FLAVOR),cannes)
diff --git a/core/arch/arm32/plat-stm/link.mk b/core/arch/arm32/plat-stm/link.mk
index 8c95bb2..f6cccc7 100644
--- a/core/arch/arm32/plat-stm/link.mk
+++ b/core/arch/arm32/plat-stm/link.mk
@@ -5,6 +5,7 @@ link-script-pp = $(link-out-dir)/tz.lds
 
 all: $(link-out-dir)/tee.elf $(link-out-dir)/tee.dmp $(link-out-dir)/tee.bin
 all: $(link-out-dir)/tee.symb_sizes
+all: $(link-out-dir)/tee-init.bin
 cleanfiles += $(link-out-dir)/tee.elf $(link-out-dir)/tee.dmp $(link-out-dir)/tee.map
 cleanfiles += $(link-out-dir)/tee.bin
 cleanfiles += $(link-out-dir)/tee.symb_sizes
@@ -35,7 +36,11 @@ $(link-out-dir)/tee.dmp: $(link-out-dir)/tee.elf
 
 $(link-out-dir)/tee.bin: $(link-out-dir)/tee.elf
 	@echo '  OBJCOPY $@'
-	$(q)$(OBJCOPY) -O binary $< $@
+	$(q)$(OBJCOPY) -O binary -j .teecore_exec $< $@
+
+$(link-out-dir)/tee-init.bin: $(link-out-dir)/tee.elf
+	@echo OBJCOPY $@
+	$(q)$(OBJCOPY) -O binary -j .teecore_init $< $@
 
 $(link-out-dir)/tee.symb_sizes: $(link-out-dir)/tee.elf
 	@echo '  GEN     $@'
diff --git a/core/arch/arm32/plat-stm/main.c b/core/arch/arm32/plat-stm/main.c
index e2d1bca..90b1405 100644
--- a/core/arch/arm32/plat-stm/main.c
+++ b/core/arch/arm32/plat-stm/main.c
@@ -50,6 +50,7 @@
 #include <kernel/tee_l2cc_mutex.h>
 #include <assert.h>
 #include <platform_config.h>
+#include <kernel/runtime_init.h>
 
 #ifdef WITH_STACK_CANARIES
 #define STACK_CANARY_SIZE	(4 * sizeof(uint32_t))
@@ -239,6 +240,12 @@ static void main_tee_entry(struct thread_smc_args *args)
 	 */
 	int ret;
 
+	/* During inits, TEE services are restricted */
+	if (!main_init_is_completed()) {
+		args->a0 = TEESMC_RETURN_EBUSY;
+		return;
+	}
+
 	/* TODO move to main_init() */
 	if (init_teecore() != TEE_SUCCESS)
 		panic();
diff --git a/core/arch/arm32/plat-stm/platform_config.h b/core/arch/arm32/plat-stm/platform_config.h
index 5e862ff..2b61f04 100644
--- a/core/arch/arm32/plat-stm/platform_config.h
+++ b/core/arch/arm32/plat-stm/platform_config.h
@@ -100,6 +100,11 @@
 #define ASC_NUM			20
 #define UART_CONSOLE_BASE	ST_ASC20_REGS_BASE
 #define RNG_BASE		0x08A89000
+#define ESRAM_BASE		0x06058000
+#define ESRAM_SIZE		0x00008000
+#define TEE_COHERENT_BASE	(ESRAM_BASE + 0x00005000)
+#define TEE_COHERENT_SIZE	(4 * 1024)
+#define TEE_SECJUMPER_BASE	(ESRAM_BASE + 0x000000FC)
 
 #elif PLATFORM_FLAVOR_IS(orly2)
 
diff --git a/core/arch/arm32/plat-stm/runtime_init.c b/core/arch/arm32/plat-stm/runtime_init.c
new file mode 100644
index 0000000..2074edb
--- /dev/null
+++ b/core/arch/arm32/plat-stm/runtime_init.c
@@ -0,0 +1,172 @@
+/*
+ * Copyright (c) 2014, STMicroelectronics International N.V.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <platform_config.h>
+#include <kernel/misc.h>
+#include <kernel/runtime_init.h>
+#include <mm/tee_mmu_defs.h>
+#include <mm/core_mmu.h>
+#include <malloc.h>
+#include <trace.h>
+#include <arm32.h>
+#include <util.h>
+
+/* the entry point for secondary cpus once primary inits are completed */
+extern void tz_runtime_init_all(void);
+
+/*
+ * Coherent RAM layout.
+ * NSec context is stored in coherent RAM before TEE MMU is setup.
+ */
+struct tee_coherent_ram_layout {
+	struct sm_nsec_ctx ctx[CFG_TEE_CORE_NB_CORE];
+};
+
+/*  MMU L2 table for coherent RAM mapping during the inits */
+static uint32_t *l2_init_coherent;
+
+/* core synchronisation */
+static int core_inited[CFG_TEE_CORE_NB_CORE];
+static int main_inited;
+
+/* get buffer in coherent RAM to save NSec ctx before cache are on */
+struct sm_nsec_ctx *main_init_get_nsec_ctx(void)
+{
+	struct tee_coherent_ram_layout *ram;
+
+	ram = (struct tee_coherent_ram_layout *)TEE_COHERENT_BASE;
+	return &ram->ctx[get_core_pos()];
+}
+
+/* main entry restrict services until all core have inited */
+unsigned long main_init_is_completed(void)
+{
+	return (main_inited == 1) ? 1 : 0;
+}
+
+/* inform that all cpus have completed their inits */
+void main_init_completed(void)
+{
+	main_inited = 1;
+}
+
+#include <kernel/asc.h>
+void main_init_logvalue(uint32_t val);
+void main_init_logvalue(uint32_t val)
+{
+        char c;
+        int i;
+
+        __asc_xmit_char('0');
+        __asc_xmit_char('x');
+        for (i = 0; i < 8; i++) {
+                c = ((val >> 28) > 9) ? 'A' + (val >> 28) - 10 : '0' + (val >> 28);
+                val <<= 4;
+                __asc_xmit_char(c);
+        }
+        __asc_xmit_char('\n');
+        __asc_flush();
+}
+
+static void init_esram_map(struct map_area *map)
+{
+	memset(map, 0, sizeof(*map));
+	map->region_size = SMALL_PAGE_SIZE;
+	map->area_pa = ROUNDDOWN(ESRAM_BASE, SECTION_SIZE);
+	map->area_size = ROUNDUP(ESRAM_BASE + ESRAM_SIZE - map->area_pa,
+				 SECTION_SIZE);
+	map->secure = true;
+	map->rw = true;
+	map->device = true;
+}
+
+/*
+ * Move NSec content from coherent RAM to target structure.
+ * Once primary cpu inits are completed, load secondary entry point.
+ * Unmap coherent RAM once all cpus have completed their inits.
+ */
+void main_init_save_nsec_ctx(void)
+{
+	unsigned int i;
+
+	if (get_core_pos() == 0) {
+		struct map_area map;
+
+		/* next mapping are all small page in ESRAM */
+		l2_init_coherent = memalign(TEE_MMU_L2_SIZE, TEE_MMU_L2_SIZE);
+		memset(l2_init_coherent, 0, TEE_MMU_L2_SIZE);
+		init_esram_map(&map);
+		map.area_l2 = l2_init_coherent;
+
+		/* map SecMonitor jumper, load secondary cores entry point */
+		map.pa = ROUNDDOWN(TEE_SECJUMPER_BASE, SMALL_PAGE_SIZE);
+		map.size = SMALL_PAGE_SIZE;
+		map_memarea(&map, (void *)core_mmu_get_main_ttb_va());
+		core_tlb_maintenance(TLBINV_UNIFIEDTLB, 0);
+
+		*(paddr_t *)TEE_SECJUMPER_BASE = (paddr_t)tz_runtime_init_all;
+		dmb();
+		unmap_memarea(&map, (void *)core_mmu_get_main_ttb_va());
+
+		/* map coherent ram during tee inits */
+		map.va = 0;
+		map.pa = TEE_COHERENT_BASE;
+		map.size = TEE_COHERENT_SIZE;
+		map_memarea(&map, (void *)core_mmu_get_main_ttb_va());
+		core_tlb_maintenance(TLBINV_UNIFIEDTLB, 0);
+	}
+
+	memcpy(sm_get_nsec_ctx(), main_init_get_nsec_ctx(),
+		sizeof(struct sm_nsec_ctx));
+
+	/* last core fuly unmaps coherent and free inits resources */
+	for (i = 0; i < CFG_TEE_CORE_NB_CORE; i++) {
+		if (i == get_core_pos())
+			continue;
+		if (core_inited[i] != 1)
+			break;
+	}
+	if (i == CFG_TEE_CORE_NB_CORE) {
+		struct map_area map;
+
+		dmb(); /* pending accesses to coherent */
+
+		/* unmap coherent ram are remove services restriction */
+		init_esram_map(&map);
+		map.area_l2 = l2_init_coherent;
+		map.pa = TEE_COHERENT_BASE;
+		map.size = TEE_COHERENT_SIZE;
+		unmap_memarea(&map, (void *)core_mmu_get_main_ttb_va());
+		core_tlb_maintenance(TLBINV_UNIFIEDTLB, 0);
+		free(l2_init_coherent);
+
+		main_init_completed();
+	} else {
+		core_inited[get_core_pos()] = 1;
+		main_init_completed();
+	}
+}
diff --git a/core/arch/arm32/plat-stm/sub.mk b/core/arch/arm32/plat-stm/sub.mk
index f4afaa3..18c3a52 100644
--- a/core/arch/arm32/plat-stm/sub.mk
+++ b/core/arch/arm32/plat-stm/sub.mk
@@ -19,3 +19,7 @@ aflags-tz_sinit.S-y += \
 	-Xassembler STM_SECONDARY_STARTUP=$(SECONDARY_STARTUP_PHYS) \
 	-Xassembler --defsym \
 	-Xassembler STEXT=$(PRIMARY_STARTUP_PHYS)
+
+srcs-y += tzl_cc_mon.S
+srcs-y += runtime_init.c
+srcs-y += tz_runtime_init.S
\ No newline at end of file
diff --git a/core/arch/arm32/plat-stm/tz-template.lds b/core/arch/arm32/plat-stm/tz-template.lds
index 285d950..cb10073 100644
--- a/core/arch/arm32/plat-stm/tz-template.lds
+++ b/core/arch/arm32/plat-stm/tz-template.lds
@@ -33,6 +33,11 @@ SECTIONS
 
     } > EXEC_MEM
 
+    .teecore_init :
+    {
+        *(.cc_common)
+    }
+
     .stacks :
     {
         __stacks_start = (.);
diff --git a/core/arch/arm32/plat-stm/tz_runtime_init.S b/core/arch/arm32/plat-stm/tz_runtime_init.S
new file mode 100644
index 0000000..a4e2c04
--- /dev/null
+++ b/core/arch/arm32/plat-stm/tz_runtime_init.S
@@ -0,0 +1,154 @@
+/*
+ * Copyright (c) 2014, STMicroelectronics International N.V.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Include(s)
+ */
+#include <kernel/tz_proc_def.h>
+#include <kernel/tz_ssvce_def.h>
+#include <sm/teesmc.h>
+#include <sm/teesmc_opteed_macros.h>
+#include <sm/teesmc_opteed.h>
+#include <arm32.h>
+#include <util.h>
+#include <asm.S>
+#include <arm32_macros.S>
+
+.global tz_runtime_init
+.global tz_runtime_init_all
+
+/*
+ * TEE firmware init sequence at run-time
+ *
+ * Entry expected in mode Monitor, spsr=NSec(cpsr), lr=NSec(pc).
+ */
+.func tz_runtime_init
+tz_runtime_init:
+
+	/* trap secondary cpus until primary init are done */
+	mrc p15, 0, r0, c0, c0, 5
+	and r1, r0, #MPIDR_CPU_MASK
+	and r0, r0, #MPIDR_CLUSTER_MASK
+	adds r0, r1, r0, lsr #6
+	beq tz_runtime_init_all
+	movs pc, lr	/* secondary returns to NSec */
+
+tz_runtime_init_all:
+	mrc p15, 0, r0, c1, c1, 0
+	tst r0, #CP15_CONFIG_NS_MASK
+	beq .
+	bic r0, r0, #CP15_CONFIG_NS_MASK
+	mcr p15, 0, r0, c1, c1, 0
+
+	/*
+	 * tz_loader leaves cpu with NSec allowed to modify ACTRL[SMP].
+	 * Set NSACR[SMP]=0 and ACTRL[SMP]=1
+	 */
+	mrc p15, 0, r0, c1, c1, 2
+	bic r0, r0, #(1 << 18)
+	mcr p15, 0, r0, c1, c1, 2
+
+	mrc p15, 0, r0, c1, c0, 1
+	orr r0, r0, #0x40
+	mcr p15, 0, r0, c1, c0, 1
+
+	/*
+	 * Save NonSecure context
+	 * - context saved to coherent RAM
+	 * - save CPSR/PC to RAM before (tmp stack, scrach r4-r5)
+	 */
+	mov r4, lr
+	bl get_core_pos
+	lsl r0, #2
+	ldr r1, =stack_tmp_top
+	ldr sp, [r1, r0]
+
+	bl main_init_get_nsec_ctx
+        mrs r5, spsr
+	push { r4, r5 }
+	mov r1, sp
+	bl sm_save_modes_regs
+	mov r4, #0
+	mov r5, #0
+	stm r0!, {r4-r12}
+
+	/*
+	 * Run inits in SVC mode (Async/FIQ/IRQ) masked
+	 */
+	cpsid aif
+	cps #CPSR_MODE_SVC
+
+	bl get_core_pos
+	lsl r2, r0, #2
+	ldr r1, =stack_tmp_top
+	ldr sp, [r1, r2]
+	cmp r0, #CPU_ID0
+	bne _secondary_cores
+
+_primary_core:
+
+	bl core_init_mmu_tables
+	bl core_init_mmu_regs
+
+	bl cpu_mmu_enable
+	bl cpu_mmu_enable_icache
+	bl cpu_mmu_enable_dcache
+
+        /* init BSS */
+	ldr r0, =__bss_start
+	ldr r2, =__bss_end
+	sub r2, r2, r0
+	ldr r1, =0
+	bl memset
+
+	b _final
+
+_secondary_cores:
+
+	bl core_init_mmu_regs
+
+	bl cpu_mmu_enable
+	bl cpu_mmu_enable_icache
+	bl cpu_mmu_enable_dcache
+
+_final:
+
+	/* tee main inits */
+	bl main_init
+
+	/* Flush all caches before secondary CPUs setup */
+	bl arm_cl1_d_cleaninvbysetway
+
+	/* copy nsec context from coherent ram to target struct */
+	bl main_init_save_nsec_ctx
+
+	/* ask monitor to enter NSec from saved NSec context */
+	mov r0, #TEESMC_OPTEED_RETURN_ENTRY_DONE
+	smc #0
+	b .	/* SMC should not return */
+
+.endfunc
diff --git a/core/arch/arm32/plat-stm/tz_sinit.S b/core/arch/arm32/plat-stm/tz_sinit.S
index df183d6..d895a44 100644
--- a/core/arch/arm32/plat-stm/tz_sinit.S
+++ b/core/arch/arm32/plat-stm/tz_sinit.S
@@ -33,6 +33,7 @@
 #include <sm/teesmc.h>
 #include <sm/teesmc_opteed_macros.h>
 #include <sm/teesmc_opteed.h>
+#include <arm32.h>
 
 /*
  * Booting on 1 or 2 cores ?
@@ -161,6 +162,11 @@ tz_sinit:
 	/* all following routines, until stack is setup, preserve R10/R11/R12 */
 #endif
 
+	mrs	r0, cpsr
+	and	r0, r0,	#CPSR_MODE_MASK
+	cmp	r0, #CPSR_MODE_MON
+	beq	tz_runtime_init
+
 	/*
 	 * Primary CPU and secondary CPUs internal initialization
 	 */
diff --git a/core/arch/arm32/plat-stm/tzl_cc_mon.S b/core/arch/arm32/plat-stm/tzl_cc_mon.S
new file mode 100644
index 0000000..bb05432
--- /dev/null
+++ b/core/arch/arm32/plat-stm/tzl_cc_mon.S
@@ -0,0 +1,67 @@
+/*
+ * Copyright (c) 2014, STMicroelectronics All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+.section .cc_common, "ax"
+.code 32
+.balign 4
+
+tz_cc_mon:
+	b .			@ reset
+	b .			@ undefined instruction
+	b smc_handler		@ secure monitor call
+	b .			@ prefetch abort
+	b .			@ data abort
+	b .			@ reserved
+	b .			@ irq
+	b .			@ fiq
+
+smc_handler:	/* starts at offset address 0x20 */
+	nop			@ offset 0x20
+	nop			@ offset 0x24
+	nop			@ offset 0x28
+	nop			@ offset 0x2C
+	nop			@ offset 0x30
+	nop			@ offset 0x34
+	nop			@ offset 0x38
+	nop			@ offset 0x3C
+	nop			@ offset 0x40
+	nop			@ offset 0x44
+	nop			@ offset 0x48
+	nop			@ offset 0x4C
+	nop			@ offset 0x50
+	nop			@ offset 0x54
+	nop			@ offset 0x58
+	nop			@ offset 0x5C
+	nop			@ offset 0x60
+	nop			@ offset 0x64
+	nop			@ offset 0x68
+	nop			@ offset 0x6C
+	nop			@ offset 0x70
+	nop			@ offset 0x74
+	ldr pc, [pc, #-4]	@ offset 0x78
+
+tee_address:
+        .word CFG_DDR_TEETZ_RESERVED_START	@ offset 0x7C
diff --git a/core/arch/arm32/sm/sm_asm.S b/core/arch/arm32/sm/sm_asm.S
index c4da5bf..f3ef399 100644
--- a/core/arch/arm32/sm/sm_asm.S
+++ b/core/arch/arm32/sm/sm_asm.S
@@ -32,7 +32,7 @@
 
 	.section .text.sm_asm
 
-LOCAL_FUNC sm_save_modes_regs , :
+FUNC sm_save_modes_regs , :
 	/* User mode registers has to be saved from system mode */
 	cps	#CPSR_MODE_SYS
 	stm	r0!, {sp, lr}
-- 
1.9.1

